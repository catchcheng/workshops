{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First let's import the necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specifying the Data Path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "file_path = os.path.join(cwd, 'cleaned_speed_dating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_df = df.loc[df['gender'] == 0]\n",
    "male_df = df.loc[df['gender'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_train, female_test = train_test_split(female_df, test_size = 0.2, random_state=42)\n",
    "male_train, male_test = train_test_split(male_df, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do logistic regression using only one variable\n",
    "predictors = ['attr_partner']\n",
    "lr_model = lr.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(lr_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(lr_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if you only use one variable, that is no different than just doing a cut off based on one of the previous graphs and make naive predictions based on that. Let's see what are the other variables that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob']\n",
    "lr_model = lr.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(lr_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(lr_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also see how important each of those factors is (sort of)\n",
    "print(lr_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it does seems like attractiveness is more indicative than anything else. Next you can try to combine them and repeat the same procedure. Is there an improvement to the performance? What can you infer from the coefficients this time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to include also those variables that you think are important and repeat the same step again. Observe what happen to the performances when you add more and more predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graphs on EDA, it seems that male and female make their decisions quite differently. Try to repeat the above with female_df and male_df and see if the results improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know how good our prediction performance is, we should at least compare it to the performances of some naive algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if I just look at the training set and guess the most popular decisions?\n",
    "no_female_train = female_train.query('dec == 0')\n",
    "print('Proportion of rejection by female in training set is {}'\\\n",
    "      .format(no_female_train.shape[0]/female_train.shape[0]))\n",
    "\n",
    "no_female_test = female_test.query('dec == 0')\n",
    "print('Proportion of rejection by female in test set is {}'\\\n",
    "      .format(no_female_test.shape[0]/female_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if I just look at the training set and guess the most popular decisions?\n",
    "no_male_train = male_train.query('dec == 0')\n",
    "print('Proportion of rejection by male in training set is {}'\\\n",
    "      .format(no_male_train.shape[0]/male_train.shape[0]))\n",
    "\n",
    "no_male_test = male_test.query('dec == 0')\n",
    "print('Proportion of rejection by male in test set is {}'\\\n",
    "      .format(no_male_test.shape[0]/male_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if I simply do a cut off at attr_partner and base my decision on that? (refer to graphs plotted in EDA)\n",
    "male_test['attr_cut_predict'] = (male_test['attr_partner']>=7)\n",
    "print('male test set performance is {}'\\\n",
    "      .format((male_test['attr_cut_predict'] == male_test['dec']).sum()/male_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_test['attr_cut_predict'] = (female_test['attr_partner']>=8)\n",
    "print('female test set performance is {}'\\\n",
    "      .format((female_test['attr_cut_predict'] == female_test['dec']).sum()/female_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree model allows combination of factors (as opposed to logistic regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(min_impurity_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob', 'attr_partner','attr_want',\n",
    " 'sinc_want',\n",
    " 'intel_want',\n",
    " 'fun_want',\n",
    " 'amb_want',\n",
    " 'shar_want']\n",
    "dt_model = dt.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(dt_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(dt_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob', 'attr_partner','attr_want',\n",
    " 'sinc_want',\n",
    " 'intel_want',\n",
    " 'fun_want',\n",
    " 'amb_want',\n",
    " 'shar_want']\n",
    "rf_model = rf.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(rf_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(rf_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob', 'attr_partner','attr_want',\n",
    " 'sinc_want',\n",
    " 'intel_want',\n",
    " 'fun_want',\n",
    " 'amb_want',\n",
    " 'shar_want']\n",
    "gb_model = gb.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(gb_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(gb_model.score(test[predictors].values, test['dec'].values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
